{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dhupee/Bangkit-C22CB-Company-Based-Capstone/blob/30b0995970f29114749cff04deef444de6832993/ML/distilbert_transfer_learn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# check python version\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook settings\n",
    "\n",
    "COLAB_MODE = False # set to True if running in Google Colab\n",
    "ENABLE_JSON2CSV = False # set to True if you want to convert json dataset to csv, DOESNT NEEDED ANYMORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if COLAB_MODE is True, then work around the repository\n",
    "if COLAB_MODE:\n",
    "    import os\n",
    "    branch_name = 'dhupee-dev'\n",
    "    cloned_repo_name = 'remote-clone'\n",
    "    target_repo_dir = '/content/remote-clone/ML'\n",
    "    repo_link = 'https://github.com/dhupee/Bangkit-C22CB-Company-Based-Capstone.git'\n",
    "    # if current directory is not the cloned repo, clone it\n",
    "    if not os.path.exists(target_repo_dir):\n",
    "        !git clone --single-branch --branch $branch_name $repo_link $cloned_repo_name\n",
    "        print('Repo successfully cloned!')\n",
    "        %cd $target_repo_dir\n",
    "        %pwd\n",
    "    else:\n",
    "        print('Repo already cloned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers and tensorflow are installed\n"
     ]
    }
   ],
   "source": [
    "# check if transformers and tensorflow are installed, if not install them\n",
    "# use transformers version 4.18.0 and tensorflow version 2.8.0\n",
    "try:\n",
    "    import transformers\n",
    "    import tensorflow as tf\n",
    "    print(\"transformers and tensorflow are installed\")\n",
    "except:\n",
    "    print(\"transformers and tensorflow are not installed\")\n",
    "    print(\"installing transformers and tensorflow\")\n",
    "    # install transformers 4.18.0 and tensorflow 2.8.0\n",
    "    %pip install transformers==4.18.0\n",
    "    %pip install tensorflow==2.8.1\n",
    "    # import transformers and tensorflow again\n",
    "    import transformers\n",
    "    import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at cahya/bert-base-indonesian-522M were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cahya/bert-base-indonesian-522M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# loading base model\n",
    "\n",
    "model_name = \"cahya/bert-base-indonesian-522M\"\n",
    "batch_size = 32\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel # make sure use tensorflow model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name) # if not specified, it will use torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  110617344 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,617,344\n",
      "Trainable params: 110,617,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 1769, 8343, 6186, 32, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tokenizer\n",
    "tokenizer(\"Nama kamu siapa?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 3245, 5366, 2464, 6014, 11186, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"saya suka makan nasi goreng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at cahya/bert-base-indonesian-522M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0840362161397934,\n",
       "  'token': 2186,\n",
       "  'token_str': 'berada',\n",
       "  'sequence': 'mainan saya berada di jalan'},\n",
       " {'score': 0.07038316130638123,\n",
       "  'token': 1821,\n",
       "  'token_str': 'ada',\n",
       "  'sequence': 'mainan saya ada di jalan'},\n",
       " {'score': 0.0403575673699379,\n",
       "  'token': 1998,\n",
       "  'token_str': 'sendiri',\n",
       "  'sequence': 'mainan saya sendiri di jalan'},\n",
       " {'score': 0.029048344120383263,\n",
       "  'token': 2444,\n",
       "  'token_str': 'lahir',\n",
       "  'sequence': 'mainan saya lahir di jalan'},\n",
       " {'score': 0.028137225657701492,\n",
       "  'token': 3812,\n",
       "  'token_str': 'berdiri',\n",
       "  'sequence': 'mainan saya berdiri di jalan'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how base model works\n",
    "unmasker = transformers.pipeline('fill-mask', model = model_name)\n",
    "unmasker(\"mainan saya [MASK] di jalan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this for converting the file to huggingface format\n",
    "#I suggest you to do it seperately\n",
    "\n",
    "def convert_huggingface(dataset_path):\n",
    "    import json\n",
    "    with open(dataset_path, encoding=\"utf8\") as f:\n",
    "        content = json.load(f)\n",
    "\n",
    "    hf_data = []\n",
    "    for data in content[\"data\"]:\n",
    "        title = data[\"title\"]\n",
    "        for paragraph in data[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                fill = {\n",
    "                    \"id\": qa[\"id\"],\n",
    "                    \"title\": title,\n",
    "                    \"context\": context,\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"answers\": {\"answer_start\": [], \"text\": []}\n",
    "                }\n",
    "                if qa[\"is_impossible\"]:\n",
    "                    answers = qa[\"plausible_answers\"]\n",
    "                else:\n",
    "                    answers = qa[\"answers\"]\n",
    "                for answer in answers:\n",
    "                    fill[\"answers\"][\"answer_start\"].append(answer[\"answer_start\"])\n",
    "                    fill[\"answers\"][\"text\"].append(answer[\"text\"])\n",
    "\n",
    "                hf_data.append(fill)\n",
    "    # Add \"_hf\" before .json extension\n",
    "    hf_dataset_path = dataset_path.replace(\".json\", \"_hf.json\")\n",
    "    with open(hf_dataset_path, \"w\") as f:\n",
    "        json.dump({\"data\": hf_data}, f)\n",
    "\n",
    "    return hf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset json file\n",
    "import json\n",
    "\n",
    "train_json_dir = \"Translated/hf_train-v2.0_indo.json\"\n",
    "valid_json_dir = \"Translated/hf_dev-v2.0_indo.json\"\n",
    "tester_json_dir  = \"Translated/tester_indo.json\"\n",
    "\n",
    "dataset_dirs = [train_json_dir, valid_json_dir, tester_json_dir]\n",
    "# dataset_dirs = [tester_json_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-75619838477d768f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:\\Users\\asus\\.cache\\huggingface\\datasets\\json\\default-75619838477d768f\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2186d06bf4f34fdfa4d68669d8099163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a8c13355ce45418ded1449f76c0251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:\\Users\\asus\\.cache\\huggingface\\datasets\\json\\default-75619838477d768f\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258d9e22de59436e98dd81af0290bdf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing dataset\n",
    "\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "except:\n",
    "    print(\"datasets module not found\")\n",
    "    print(\"installing dataset module\")\n",
    "    %pip install datasets\n",
    "    from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset(\n",
    "    'json',\n",
    "    data_files={'train': train_json_dir, 'validation': valid_json_dir},\n",
    "    field='data'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56bf6b0f3aeaaa14008c9602',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/ biːˈjɒnseɪ / bee-YON-say) (lahir 4 September 1981) adalah seorang penyanyi, penulis lagu, produser rekaman dan aktris Amerika. Dilahirkan dan dibesarkan di Houston, Texas, ia tampil di berbagai kompetisi menyanyi dan menari sebagai seorang anak, dan mulai terkenal pada akhir 1990-an sebagai penyanyi utama dari grup wanita R&B Destiny\\'s Child. Dikelola oleh ayahnya, Mathew Knowles, grup ini menjadi salah satu grup wanita terlaris di dunia sepanjang masa. Jeda mereka melihat perilisan album debut Beyoncé, Dangerously in Love (2003), yang menjadikannya sebagai artis solo di seluruh dunia, meraih lima Grammy Awards dan menampilkan single nomor satu Billboard Hot 100 \"Crazy in Love\" dan \"Baby Boy\" .',\n",
       " 'question': 'Pada dekade berapa Beyonce menjadi terkenal?',\n",
       " 'answers': {'answer_start': [304], 'text': ['akhir 1990-an']}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample of dataset randomly\n",
    "import random\n",
    "datasets[\"train\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56ddde6b9a695914005b962c',\n",
       " 'title': 'orang Normandia',\n",
       " 'context': 'Bangsa Norman (Norman: Nourmands; Prancis: Normandia; Latin: Normanni) adalah orang-orang yang pada abad ke-10 dan ke-11 memberi nama kepada Normandia, sebuah wilayah di Prancis. Mereka adalah keturunan dari perampok dan bajak laut Norse (\"Norman\" berasal dari \"Norseman\") dan bajak laut dari Denmark, Islandia dan Norwegia yang, di bawah pemimpin mereka Rollo, setuju untuk bersumpah setia kepada Raja Charles III dari Francia Barat. Melalui generasi asimilasi dan pencampuran dengan penduduk asli Franka dan Romawi-Gaul, keturunan mereka secara bertahap akan bergabung dengan budaya berbasis Carolingian di Francia Barat. Identitas budaya dan etnis yang berbeda dari Normandia awalnya muncul pada paruh pertama abad ke-10, dan terus berkembang selama abad-abad berikutnya.',\n",
       " 'question': 'Abad berapa orang Normandia pertama kali mendapatkan identitas mereka yang terpisah?',\n",
       " 'answers': {'answer_start': [671, 649, 671, 671],\n",
       "  'text': ['abad ke-10',\n",
       "   'paruh pertama abad ke-10',\n",
       "   'tanggal 10',\n",
       "   'tanggal 10']}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample of dataset randomly\n",
    "import random\n",
    "datasets[\"validation\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    '''display random elements from dataset\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): dataset to show\n",
    "        num_examples (int, optional): number of examples to show. Defaults to 10.\n",
    "    '''\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(\n",
    "                lambda x: [typ.feature.names[i] for i in x]\n",
    "            )\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>572a054f6aef0514001551b7</td>\n",
       "      <td>East_Prussia</td>\n",
       "      <td>Prusia Timur menutupi sebagian besar tanah leluhur Prusia Kuno Baltik. Selama abad ke-13, penduduk asli Prusia ditaklukkan oleh Tentara Salib para Ksatria Teutonik. Suku Balt asli yang selamat dari penaklukan secara bertahap menjadi Kristen. Karena Jermanisasi dan penjajahan selama abad-abad berikutnya, Jerman menjadi kelompok etnis yang dominan, sedangkan Polandia dan Lituania menjadi minoritas. Dari abad ke-13, Prusia Timur adalah bagian dari negara biara Ksatria Teutonik. Setelah Kedamaian Duri Kedua pada tahun 1466 menjadi wilayah kekuasaan Kerajaan Polandia. Pada tahun 1525, dengan Penghormatan Prusia, provinsi tersebut menjadi Kadipaten Prusia. Bahasa Prusia Kuno telah punah pada abad ke-17 atau awal abad ke-18.</td>\n",
       "      <td>Apa kelompok lain selama periode ini untuk minoritas bentuk?</td>\n",
       "      <td>{'answer_start': [359], 'text': ['Polandia dan Lituania']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5ad33f45604f3c001a3fdbc5</td>\n",
       "      <td>PlayStation_3</td>\n",
       "      <td>Pada E3 2007, Sony mampu menampilkan sejumlah video game mendatang mereka untuk PlayStation 3, termasuk Heavenly Sword, Lair, Ratchet &amp; Clank Future: Tools of Destruction, Warhawk dan Uncharted: Drake's Fortune; semuanya dirilis pada kuartal ketiga dan keempat tahun 2007. Mereka juga memamerkan sejumlah judul yang akan dirilis pada tahun 2008 dan 2009; terutama Killzone 2, Infamous, Gran Turismo 5 Prologue, LittleBigPlanet dan SOCOM: U.S. Navy SEALs Confrontation. Sejumlah eksklusif pihak ketiga juga ditampilkan, termasuk Metal Gear Solid 4: Guns of the Patriots yang sangat dinanti, bersama judul pihak ketiga profil tinggi lainnya seperti Grand Theft Auto IV, Call of Duty 4: Modern Warfare, Assassin's Creed, Devil May Cry 4 dan Resident Evil 5. Dua judul penting lainnya untuk PlayStation 3, Final Fantasy XIII dan Final Fantasy Versus XIII, ditampilkan di TGS 2007 untuk menenangkan pasar Jepang.</td>\n",
       "      <td>Game pihak ketiga yang paling dinantikan dengan nama bulan dalam tahun di dalamnya yang ditampilkan Sony di E4 2007?</td>\n",
       "      <td>{'answer_start': [718], 'text': ['Devil May Cry 4']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56e7af6400c9c71400d774e0</td>\n",
       "      <td>Nanjing</td>\n",
       "      <td>Saat ini, dengan tradisi budaya yang panjang dan dukungan kuat dari lembaga pendidikan setempat, Nanjing umumnya dipandang sebagai \"kota budaya\" dan salah satu kota yang lebih menyenangkan untuk ditinggali di Tiongkok.</td>\n",
       "      <td>Siapa yang memberikan dukungan kuat kepada Nanjing?</td>\n",
       "      <td>{'answer_start': [68], 'text': ['lembaga pendidikan setempat,']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56dfc1797aa994140058e12e</td>\n",
       "      <td>Lighting</td>\n",
       "      <td>The Professional Lighting And Sound Association (PLASA) adalah organisasi perdagangan berbasis di Inggris yang mewakili 500+ anggota individu dan perusahaan yang diambil dari sektor layanan teknis. Anggotanya termasuk produsen dan distributor pencahayaan panggung dan hiburan, suara, tali-temali dan produk dan layanan serupa, serta profesional terafiliasi di area tersebut. Mereka melobi dan mewakili kepentingan industri di berbagai tingkatan, berinteraksi dengan pemerintah dan badan pengatur dan mempresentasikan kasus untuk industri hiburan. Contoh subjek dari representasi ini termasuk peninjauan frekuensi radio yang sedang berlangsung (yang mungkin atau mungkin tidak memengaruhi pita radio yang digunakan mikrofon nirkabel dan perangkat lain) dan terlibat dengan masalah seputar pengenalan peraturan RoHS (Restriction of Hazardous Substances Directive) .</td>\n",
       "      <td>Apa singkatan dari RoHS?</td>\n",
       "      <td>{'answer_start': [814], 'text': ['(Restriction of Hazardous']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5acd153607355d001abf33ce</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Energi total suatu sistem dapat dibagi lagi dan diklasifikasikan dalam berbagai cara. Misalnya, mekanika klasik membedakan antara energi kinetik yang ditentukan oleh pergerakan benda melalui ruang, dan energi potensial, yang merupakan fungsi dari posisi suatu benda dalam suatu medan. Mungkin juga mudah untuk membedakan energi gravitasi, energi panas, beberapa jenis energi nuklir (yang memanfaatkan potensial dari gaya nuklir dan gaya lemah), energi listrik (dari medan listrik), dan energi magnet (dari medan magnet) , diantara yang lain. Banyak dari klasifikasi ini tumpang tindih; misalnya, energi panas biasanya terdiri dari energi kinetik dan sebagian lagi energi potensial.</td>\n",
       "      <td>klasifikasi apa yang tidak tumpang tindih?</td>\n",
       "      <td>{'answer_start': [596], 'text': ['energi panas biasanya terdiri dari energi kinetik dan sebagian lagi energi potensial.']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>572840b1ff5b5019007da004</td>\n",
       "      <td>Federalism</td>\n",
       "      <td>Federasi sering menggunakan paradoks sebagai penyatuan negara, sementara tetap menjadi negara (atau memiliki aspek kenegaraan) dalam dirinya sendiri. Misalnya, James Madison (penulis Konstitusi AS) menulis dalam Federalist Paper No. 39 bahwa Konstitusi AS \"dalam ketegasannya bukanlah konstitusi nasional maupun federal; tetapi komposisi keduanya. Pada dasarnya, itu federal, bukan nasional; dalam sumber dari mana kekuasaan biasa Pemerintah diambil, itu sebagian federal, dan sebagian nasional ... \"Ini berasal dari fakta bahwa negara bagian di AS mempertahankan semua kedaulatan sehingga mereka tidak menyerah kepada federasi dengan persetujuan mereka sendiri. Hal ini ditegaskan kembali oleh Amandemen Kesepuluh atas Konstitusi Amerika Serikat, yang memiliki semua kekuasaan dan hak yang tidak dilimpahkan kepada Pemerintah Federal sebagaimana diserahkan kepada Amerika Serikat dan rakyat.</td>\n",
       "      <td>Apa dasar dari kertas federalis no. 39?</td>\n",
       "      <td>{'answer_start': [363], 'text': ['itu federal, bukan nasional; dalam sumber dari mana kekuasaan biasa Pemerintah diambil, itu sebagian federal, dan sebagian nasional ...']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5a863e3ab4e223001a8e7503</td>\n",
       "      <td>Russian_language</td>\n",
       "      <td>Dialek Rusia Utara dan yang diucapkan di sepanjang Sungai Volga biasanya diucapkan tanpa tekanan / o / dengan jelas (fenomena ini disebut okanye / оканье). Selain tidak adanya reduksi vokal, beberapa dialek memiliki tinggi atau diftong / e ~ i̯ɛ / menggantikan Proto-Slavia * ě dan / o ~ u̯ɔ / dalam suku kata tertutup yang ditekankan (seperti dalam bahasa Ukraina), bukan Bahasa Rusia Standar / e / dan /Hai/. Ciri morfologi yang menarik adalah artikel pasti pasca-posed -to, -ta, -te mirip dengan yang ada di Bulgaria dan Makedonia.</td>\n",
       "      <td>Apa dialek yang digunakan di Makedonia?</td>\n",
       "      <td>{'answer_start': [7], 'text': ['Rusia Utara']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5acd3fbb07355d001abf3a43</td>\n",
       "      <td>Estonian_language</td>\n",
       "      <td>Meskipun ortografi Estonia umumnya dipandu oleh prinsip fonemik, dengan setiap grafem berhubungan dengan satu fonem, terdapat beberapa penyimpangan historis dan morfologis dari ini: misalnya pelestarian morfem dalam deklarasi kata (menulis b, g, d di tempat-tempat di mana p, k, t diucapkan) dan dalam penggunaan 'i' dan 'j'. [klarifikasi diperlukan] Jika sangat tidak praktis atau tidak mungkin untuk mengetikkan š dan ž, mereka diganti dengan sh dan zh dalam beberapa teks tertulis, meskipun ini dianggap tidak benar. Jika tidak, h in sh mewakili frikatif glotal yang tidak bersuara, seperti dalam Pasha (pas-ha); ini juga berlaku untuk beberapa nama asing.</td>\n",
       "      <td>Pada kesempatan apa š dan ž diganti dengan ch dan zu?</td>\n",
       "      <td>{'answer_start': [356], 'text': ['sangat tidak praktis atau tidak mungkin untuk mengetikkan š dan ž,']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56de8c374396321400ee2a14</td>\n",
       "      <td>Arnold_Schwarzenegger</td>\n",
       "      <td>Twins (1988), sebuah komedi dengan Danny DeVito, juga terbukti sukses. Total Recall (1990) menjaring Schwarzenegger $ 10 juta dan 15% dari pendapatan kotor film. Sebuah naskah fiksi ilmiah, film ini didasarkan pada cerita pendek Philip K. Dick \"We Can Remember It for You Wholesale\". Kindergarten Cop (1990) mempertemukannya kembali dengan sutradara Ivan Reitman, yang mengarahkannya di Twins. Schwarzenegger memiliki pengalaman singkat dalam penyutradaraan, pertama dengan episode tahun 1990 dari serial TV Tales from the Crypt, berjudul \"The Switch\", dan kemudian dengan film televisi tahun 1992 Natal di Connecticut. Dia tidak menyutradarai sejak itu.</td>\n",
       "      <td>Sebuah episode dari serial TV terkenal apa yang merupakan debut sutradara Schwarzenegger?</td>\n",
       "      <td>{'answer_start': [519], 'text': ['the Crypt, berjudul']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5a299c8103c0e7001a3e1858</td>\n",
       "      <td>United_Nations_Population_Fund</td>\n",
       "      <td>Presiden Bush menolak dana untuk UNFPA. Selama masa Pemerintahan Bush, sejumlah $ 244 juta dalam pendanaan yang disetujui Kongres diblokir oleh Cabang Eksekutif.</td>\n",
       "      <td>Pejabat pemerintah mana yang meningkatkan pendanaan untuk UNFPA?</td>\n",
       "      <td>{'answer_start': [0], 'text': ['Presiden']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"]) # brace yourself, this is gonna be a long list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function is for converting SQuAD json file to pandas dataframe, iteratively\n",
    "\n",
    "    I dont want run this locally, better use colab\n",
    "\n",
    "    doesn't needed anymore, use load_dataset instead\n",
    "'''\n",
    "\n",
    "if ENABLE_JSON2CSV:\n",
    "    import utils\n",
    "    for dir in dataset_dirs:\n",
    "        with open(dir, encoding=\"utf-8\") as json_file:\n",
    "            file = json.load(json_file)\n",
    "            dict_file = file\n",
    "            data = dict_file['data']\n",
    "\n",
    "        df = utils.json_to_df(data)\n",
    "        df.to_csv(dir.replace(\".json\", \".csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast) # make sure tokenizer is pre-trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384  # The maximum length of a feature (question and context)\n",
    "doc_stride = 128  # The allowed overlap between two part of the context when splitting is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there's dataset feature\n",
    "# longer than max_length\n",
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "example = datasets[\"train\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there's dataset feature longer than max_length\n",
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check truncate dataset length\n",
    "len(\n",
    "    tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "    )[\"input_ids\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 156]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] beyonce menikah pada 2008 dengan siapa? [SEP] pada 4 april 2008, beyonce menikahi jay z. dia secara terbuka mengungkapkan pernikahan mereka dalam montase video di pesta mendengarkan untuk album studio ketiganya, i am... sasha fierce, di sony club manhattan pada 22 oktober 2008. i am... sasha fierce dirilis pada 18 november 2008 di amerika serikat. album ini secara resmi memperkenalkan alter ego beyonce sasha fierce, yang dibuat selama pembuatan singel tahun 2003 \" crazy in love \", terjual 482. 000 kopi di minggu pertama, memulai debutnya di atas billboard 200, dan memberikan beyonce album nomor satu ketiganya berturut - turut di kami. album ini menampilkan lagu nomor satu \" single ladies ( put a ring on it ) \" dan lagu lima teratas \" if i were a boy \" dan \" halo \". mencapai pencapaian menjadi single hot 100 terlama dalam karirnya, kesuksesan \" halo \" di as membantu beyonce mencapai lebih dari sepuluh single teratas dalam daftar daripada wanita lain selama tahun 2000 - an. ini juga termasuk \" mimpi manis \" yang sukses, dan single \" diva \", \" ego \", \" gadis patah hati \" dan \" telepon video \". video musik untuk \" single ladies \" telah diparodikan dan ditiru di seluruh dunia, memicu \" kegilaan tari besar pertama \" di era internet menurut toronto star. video ini telah memenangkan beberapa penghargaan, termasuk video terbaik di penghargaan musik eropa mtv 2009, penghargaan mobo skotlandia 2009, dan penghargaan bet 2009. pada mtv video music awards 2009, video tersebut dinominasikan untuk sembilan penghargaan, akhirnya memenangkan tiga penghargaan termasuk video of the year. kegagalannya untuk memenangkan kategori video wanita terbaik, yang jatuh ke lagu \" you belong with me \" dari penyanyi country amerika taylor swift, menyebabkan kanye west menyela upacara dan beyonce mengimprovisasi presentasi ulang penghargaan swift selama pidato penerimaannya sendiri. pada bulan maret 2009, beyonce memulai i am... world tour, [SEP]\n",
      "[CLS] beyonce menikah pada 2008 dengan siapa? [SEP]an tari besar pertama \" di era internet menurut toronto star. video ini telah memenangkan beberapa penghargaan, termasuk video terbaik di penghargaan musik eropa mtv 2009, penghargaan mobo skotlandia 2009, dan penghargaan bet 2009. pada mtv video music awards 2009, video tersebut dinominasikan untuk sembilan penghargaan, akhirnya memenangkan tiga penghargaan termasuk video of the year. kegagalannya untuk memenangkan kategori video wanita terbaik, yang jatuh ke lagu \" you belong with me \" dari penyanyi country amerika taylor swift, menyebabkan kanye west menyela upacara dan beyonce mengimprovisasi presentasi ulang penghargaan swift selama pidato penerimaannya sendiri. pada bulan maret 2009, beyonce memulai i am... world tour, tur konser dunia keduanya, yang terdiri dari 108 pertunjukan, meraup $ 119, 5 juta. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 7), (8, 15), (16, 20), (21, 25), (26, 32), (33, 38), (38, 39), (0, 0), (0, 4), (5, 6), (7, 12), (13, 17), (17, 18), (19, 26), (27, 35), (36, 39), (40, 41), (41, 42), (43, 46), (47, 53), (54, 61), (62, 75), (76, 86), (87, 93), (94, 99), (100, 104), (104, 107), (108, 113), (114, 116), (117, 122), (123, 135), (136, 141), (142, 147), (148, 154), (155, 164), (164, 165), (166, 167), (168, 170), (171, 172), (172, 173), (173, 174), (175, 178), (178, 180), (181, 183), (183, 186), (186, 187), (187, 188), (189, 191), (192, 196), (197, 201), (202, 211), (212, 216), (217, 219), (220, 227), (228, 232), (232, 233), (234, 235), (236, 238), (239, 240), (240, 241), (241, 242), (243, 246), (246, 248), (249, 251), (251, 254), (254, 255), (256, 263), (264, 268), (269, 271), (272, 280), (281, 285), (286, 288), (289, 296), (297, 304), (304, 305), (306, 311), (312, 315), (316, 322), (323, 328), (329, 343), (344, 349), (350, 353), (354, 361), (362, 365), (365, 367), (368, 370), (370, 373), (373, 374), (374, 375), (376, 380), (381, 387), (388, 394), (395, 404), (405, 411), (412, 417), (418, 422), (423, 424), (424, 429), (430, 432)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride,\n",
    ")\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some layers of TFBertForQuestionAnswering were not initialized from the model checkpoint at cahya/bert-base-indonesian-522M and are newly initialized: ['qa_outputs']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 17\n"
     ]
    }
   ],
   "source": [
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# Start token index of the current span in the text.\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# End token index of the current span in the text.\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (\n",
    "    offsets[token_start_index][0] <= start_char\n",
    "    and offsets[token_end_index][1] >= end_char\n",
    "):\n",
    "    # Move the token_start_index and token_end_index to the two ends of the answer.\n",
    "    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "    while (\n",
    "        token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char\n",
    "    ):\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"The answer is not in this feature.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jay z\n",
      "Jay Z\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.decode(\n",
    "        tokenized_example[\"input_ids\"][0][start_position : end_position + 1]\n",
    "    )\n",
    ")\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (\n",
    "                offsets[token_start_index][0] <= start_char\n",
    "                and offsets[token_end_index][1] >= end_char\n",
    "            ):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while (\n",
    "                    token_start_index < len(offsets)\n",
    "                    and offsets[token_start_index][0] <= start_char\n",
    "                ):\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = prepare_train_features(datasets[\"train\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [01:40<00:00,  1.14ba/s]\n",
      "100%|██████████| 12/12 [00:10<00:00,  1.18ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "    prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some layers of TFBertForQuestionAnswering were not initialized from the model checkpoint at cahya/bert-base-indonesian-522M and are newly initialized: ['qa_outputs']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_name.split(\"/\")[-1]\n",
    "push_to_hub_model_id = f\"{model_name}-finetuned-squad\"\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 2\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"start_positions\", \"end_positions\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "validation_set = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"start_positions\", \"end_positions\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "total_train_steps = (len(tokenized_datasets[\"train\"]) // batch_size) * num_train_epochs\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=learning_rate, num_warmup_steps=0, num_train_steps=total_train_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./qa_model_save/logs\")\n",
    "\n",
    "callbacks = [tensorboard_callback]\n",
    "\n",
    "model.fit(\n",
    "    train_set,\n",
    "    validation_data=validation_set,\n",
    "    epochs=num_train_epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(validation_set))\n",
    "output = model.predict_on_batch(batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(output.start_logits, -1), np.argmax(output.end_logits, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0]\n",
    "end_logits = output.end_logits[0]\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if (\n",
    "            start_index <= end_index\n",
    "        ):  # We need to refine that test to check the answer is inside the context\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\",  # We need to find a way to get back the original substring corresponding to the answer in the context\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = validation_features.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions = model.predict(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = output.start_logits[0]\n",
    "end_logits = output.end_logits[0]\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n",
    "# an example index\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "        # to part of the input_ids that are not in the context.\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if (\n",
    "            start_index <= end_index\n",
    "        ):  # We need to refine that test to check the answer is inside the context\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char:end_char],\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[\n",
    "    :n_best_size\n",
    "]\n",
    "valid_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49ab837104032442cb3286ae94c136228588f7f161e08994a65f33f02a880070"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
