{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dhupee/Bangkit-C22CB-Company-Based-Capstone/blob/30b0995970f29114749cff04deef444de6832993/ML/distilbert_transfer_learn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# check python version\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook settings\n",
    "\n",
    "COLAB_MODE = False # set to True if running in Google Colab\n",
    "ENABLE_JSON2CSV = False # set to True if you want to convert json dataset to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if COLAB_MODE is True, then work around the repository\n",
    "if COLAB_MODE:\n",
    "    import os\n",
    "    branch_name = 'dhupee-dev'\n",
    "    cloned_repo_name = 'remote-clone'\n",
    "    target_repo_dir = '/content/remote-clone/ML'\n",
    "    repo_link = 'https://github.com/dhupee/Bangkit-C22CB-Company-Based-Capstone.git'\n",
    "    # if current directory is not the cloned repo, clone it\n",
    "    if not os.path.exists(target_repo_dir):\n",
    "        !git clone --single-branch --branch $branch_name $repo_link $cloned_repo_name\n",
    "        print('Repo successfully cloned!')\n",
    "        %cd $target_repo_dir\n",
    "        %pwd\n",
    "    else:\n",
    "        print('Repo already cloned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers and tensorflow are installed\n"
     ]
    }
   ],
   "source": [
    "# check if transformers and tensorflow are installed, if not install them\n",
    "# use transformers version 4.18.0 and tensorflow version 2.8.0\n",
    "try:\n",
    "    import transformers\n",
    "    import tensorflow as tf\n",
    "    print(\"transformers and tensorflow are installed\")\n",
    "except:\n",
    "    print(\"transformers and tensorflow are not installed\")\n",
    "    print(\"installing transformers and tensorflow\")\n",
    "    # install transformers 4.18.0 and tensorflow 2.8.0\n",
    "    %pip install transformers==4.18.0\n",
    "    %pip install tensorflow==2.8.0\n",
    "    # import transformers and tensorflow again\n",
    "    import transformers\n",
    "    import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at cahya/bert-base-indonesian-522M were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cahya/bert-base-indonesian-522M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"cahya/bert-base-indonesian-522M\"\n",
    "batch_size = 32\n",
    "\n",
    "from transformers import BertTokenizer, TFAutoModel # make sure use tensorflow model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  110617344 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,617,344\n",
      "Trainable params: 110,617,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 1769, 8343, 6186, 32, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tokenizer\n",
    "tokenizer(\"Nama kamu siapa?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 3245, 5366, 2464, 6014, 11186, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"saya suka makan nasi goreng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at cahya/bert-base-indonesian-522M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0840364545583725,\n",
       "  'token': 2186,\n",
       "  'token_str': 'berada',\n",
       "  'sequence': 'mainan saya berada di jalan'},\n",
       " {'score': 0.07038316130638123,\n",
       "  'token': 1821,\n",
       "  'token_str': 'ada',\n",
       "  'sequence': 'mainan saya ada di jalan'},\n",
       " {'score': 0.0403575636446476,\n",
       "  'token': 1998,\n",
       "  'token_str': 'sendiri',\n",
       "  'sequence': 'mainan saya sendiri di jalan'},\n",
       " {'score': 0.029048316180706024,\n",
       "  'token': 2444,\n",
       "  'token_str': 'lahir',\n",
       "  'sequence': 'mainan saya lahir di jalan'},\n",
       " {'score': 0.028137197718024254,\n",
       "  'token': 3812,\n",
       "  'token_str': 'berdiri',\n",
       "  'sequence': 'mainan saya berdiri di jalan'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = transformers.pipeline('fill-mask', model = model_name)\n",
    "unmasker(\"mainan saya [MASK] di jalan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset json file\n",
    "import json\n",
    "\n",
    "train_json_dir = \"Translated/train-v2.0_indo.json\"\n",
    "dev_json_dir = \"Translated/dev-v2.0_indo.json\"\n",
    "tester_json_dir  = \"Translated/tester_indo.json\"\n",
    "\n",
    "dataset_dirs = [train_json_dir, dev_json_dir, tester_json_dir]\n",
    "# dataset_dirs = [tester_json_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function is for converting SQuAD json file to pandas dataframe, iteratively\n",
    "\n",
    "    I dont want run this locally, better use colab\n",
    "'''\n",
    "\n",
    "if ENABLE_JSON2CSV:\n",
    "    import utils\n",
    "    for dir in dataset_dirs:\n",
    "        with open(dir, encoding=\"utf-8\") as json_file:\n",
    "            file = json.load(json_file)\n",
    "            dict_file = file\n",
    "            data = dict_file['data']\n",
    "\n",
    "        df = utils.json_to_df(data)\n",
    "        df.to_csv(dir.replace(\".json\", \".csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384  # The maximum length of a feature (question and context)\n",
    "doc_stride = 128  # The allowed overlap between two part of the context when splitting is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Di negara manakah Normandia berada?</td>\n",
       "      <td>Perancis</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Di negara manakah Normandia berada?</td>\n",
       "      <td>Perancis</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Di negara manakah Normandia berada?</td>\n",
       "      <td>Perancis</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Di negara manakah Normandia berada?</td>\n",
       "      <td>Perancis</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56ddde6b9a695914005b9629</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Kapan orang Normandia di Normandia?</td>\n",
       "      <td>abad 10 dan 11</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   input_id            title  \\\n",
       "0  56ddde6b9a695914005b9628  orang Normandia   \n",
       "1  56ddde6b9a695914005b9628  orang Normandia   \n",
       "2  56ddde6b9a695914005b9628  orang Normandia   \n",
       "3  56ddde6b9a695914005b9628  orang Normandia   \n",
       "4  56ddde6b9a695914005b9629  orang Normandia   \n",
       "\n",
       "                                             context  \\\n",
       "0  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "1  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "2  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "3  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "4  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "\n",
       "                              question     answer_text  answer_start  \n",
       "0  Di negara manakah Normandia berada?        Perancis         159.0  \n",
       "1  Di negara manakah Normandia berada?        Perancis         159.0  \n",
       "2  Di negara manakah Normandia berada?        Perancis         159.0  \n",
       "3  Di negara manakah Normandia berada?        Perancis         159.0  \n",
       "4  Kapan orang Normandia di Normandia?  abad 10 dan 11          94.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    print(\"pandas is not installed\")\n",
    "    print(\"installing pandas...\")\n",
    "    %pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "train_csv_dir = \"Translated/train-v2.0_indo.csv\"\n",
    "dev_csv_dir = \"Translated/dev-v2.0_indo.csv\"\n",
    "tester_csv_dir  = \"Translated/tester_indo.csv\"\n",
    "\n",
    "# open one of the csv file\n",
    "df_tester = pd.read_csv(tester_csv_dir)\n",
    "df_tester.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input_id         object\n",
       "title            object\n",
       "context          object\n",
       "question         object\n",
       "answer_text      object\n",
       "answer_start    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check dataframe datatype\n",
    "df_tester.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated/train-v2.0_indo.csv has no features longer than 384\n",
      "\n",
      "\n",
      "Translated/dev-v2.0_indo.csv has no features longer than 384\n",
      "\n",
      "\n",
      "Translated/tester_indo.csv has no features longer than 384\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dirs = [train_csv_dir, dev_csv_dir, tester_csv_dir]\n",
    "\n",
    "# check every csv file, if there is feature longer than max_length, then print it\n",
    "for csv_dir in csv_dirs:\n",
    "    df = pd.read_csv(csv_dir)\n",
    "    longer_feature = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if len(row[['context', 'question', 'input_id']]) > max_length:\n",
    "            longer_feature += 1\n",
    "            print(f\"{csv_dir} has {longer_feature} features longer than {max_length}\")\n",
    "            print(\"\\n\")\n",
    "    if longer_feature == 0:\n",
    "        print(f\"{csv_dir} has no features longer than {max_length}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some layers of TFBertForQuestionAnswering were not initialized from the model checkpoint at cahya/bert-base-indonesian-522M and are newly initialized: ['qa_outputs']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "num_train_epochs = 2\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load preprocessed dataset\n",
    "# TODO: train model\n",
    "# TODO: evaluate model\n",
    "# TODO: save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load preprocessed dataset\n",
    "train_csv_dir = \"Translated/train-v2.0_indo.csv\"\n",
    "dev_csv_dir = \"Translated/dev-v2.0_indo.csv\"\n",
    "\n",
    "# load preprocessed dataset\n",
    "df_train = pd.read_csv(train_csv_dir)\n",
    "df_dev = pd.read_csv(dev_csv_dir)\n",
    "\n",
    "def get_dataset(df, batch_size):\n",
    "    \"\"\"\n",
    "    This function is for converting pandas dataframe to tf.data.Dataset\n",
    "    \"\"\"\n",
    "    def _parse_function(input_id, question, context):\n",
    "        input_id = tf.cast(input_id, tf.int32)\n",
    "        question = tf.cast(question, tf.int32)\n",
    "        context = tf.cast(context, tf.int32)\n",
    "        return input_id, question, context\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((df['input_id'], df['question'], df['context']))\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "# train dataset\n",
    "train_dataset = get_dataset(df_train, batch_size)\n",
    "\n",
    "# dev dataset\n",
    "dev_dataset = get_dataset(df_dev, batch_size)\n",
    "\n",
    "# train model\n",
    "def train_model(model, train_dataset, dev_dataset, learning_rate, num_train_epochs, weight_decay):\n",
    "    \"\"\"\n",
    "    This function is for training model\n",
    "    \"\"\"\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate = learning_rate,\n",
    "    )\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_train_epochs}\")\n",
    "        print('-' * 10)\n",
    "\n",
    "        for step, batch in enumerate(train_dataset):\n",
    "            loss_value, gradients, gradient_norm = loss.get_loss_and_gradients(batch, model)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Step {step + 1}/{len(train_dataset)}, loss: {loss_value}\")\n",
    "\n",
    "        # evaluate model\n",
    "        print(\"\\nEvaluating model on dev set...\")\n",
    "        print('-' * 10)\n",
    "        dev_loss, dev_accuracy = evaluate_model(model, dev_dataset)\n",
    "        print(f\"Dev set: loss: {dev_loss}, accuracy: {dev_accuracy}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# evaluate model\n",
    "def evaluate_model(model, dataset):\n",
    "    \"\"\"\n",
    "    This function is for evaluating model\n",
    "    \"\"\"\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for batch in dataset:\n",
    "        loss_value, _, _ = loss.get_loss_and_gradients(batch, model)\n",
    "        total_loss += loss_value\n",
    "        num_batches += 1\n",
    "\n",
    "        predictions = model(batch)\n",
    "        num_correct += tf.math.count_nonzero(\n",
    "            tf.math.equal(\n",
    "                predictions.start_logits,\n",
    "                tf.math.argmax(batch.start_positions, axis = -1)\n",
    "            )\n",
    "        )\n",
    "        num_correct += tf.math.count_nonzero(\n",
    "            tf.math.equal(\n",
    "                predictions.end_logits,\n",
    "                tf.math.argmax(batch.end_positions, axis = -1)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    accuracy = num_correct / (2 * len(dataset))\n",
    "\n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "model = train_model(model, train_dataset, dev_dataset, learning_rate, num_train_epochs, weight_decay)\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7f8b4dcb172d5ee7cbc6306f910f9536d12fce62ed018c909e0bf052dc4ebfc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
