{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# check python version\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook settings\n",
    "\n",
    "COLAB_MODE = False # set to True if running in Google Colab\n",
    "ENABLE_JSON2CSV = False # set to True if you want to convert json dataset to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if COLAB_MODE is True, then work around the repository\n",
    "if COLAB_MODE:\n",
    "    import os\n",
    "    branch_name = 'dhupee-dev'\n",
    "    cloned_repo_name = 'remote-clone'\n",
    "    target_repo_dir = '/content/remote-clone/ML'\n",
    "    repo_link = 'https://github.com/dhupee/Bangkit-C22CB-Company-Based-Capstone.git'\n",
    "    # if current directory is not the cloned repo, clone it\n",
    "    if not os.path.exists(target_repo_dir):\n",
    "        !git clone --single-branch --branch $branch_name $repo_link $cloned_repo_name\n",
    "        print('Repo successfully cloned!')\n",
    "        %cd $target_repo_dir\n",
    "        %pwd\n",
    "    else:\n",
    "        print('Repo already cloned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers and tensorflow are installed\n"
     ]
    }
   ],
   "source": [
    "# check if transformers and tensorflow are installed, if not install them\n",
    "# use transformers version 4.18.0 and tensorflow version 2.8.0\n",
    "try:\n",
    "    import transformers\n",
    "    import tensorflow as tf\n",
    "    print(\"transformers and tensorflow are installed\")\n",
    "except:\n",
    "    print(\"transformers and tensorflow are not installed\")\n",
    "    print(\"installing transformers and tensorflow\")\n",
    "    # install transformers 4.18.0 and tensorflow 2.8.0\n",
    "    %pip install transformers==4.18.0\n",
    "    %pip install tensorflow==2.8.0\n",
    "    # import transformers and tensorflow again\n",
    "    import transformers\n",
    "    import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at cahya/bert-base-indonesian-522M were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cahya/bert-base-indonesian-522M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"cahya/bert-base-indonesian-522M\"\n",
    "batch_size = 32\n",
    "\n",
    "from transformers import BertTokenizer, TFAutoModel # make sure use tensorflow model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  110617344 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,617,344\n",
      "Trainable params: 110,617,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 1769, 8343, 6186, 32, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tokenizer\n",
    "tokenizer(\"Nama kamu siapa?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 3245, 5366, 2464, 6014, 11186, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"saya suka makan nasi goreng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at cahya/bert-base-indonesian-522M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0840364545583725,\n",
       "  'token': 2186,\n",
       "  'token_str': 'berada',\n",
       "  'sequence': 'mainan saya berada di jalan'},\n",
       " {'score': 0.07038316130638123,\n",
       "  'token': 1821,\n",
       "  'token_str': 'ada',\n",
       "  'sequence': 'mainan saya ada di jalan'},\n",
       " {'score': 0.0403575636446476,\n",
       "  'token': 1998,\n",
       "  'token_str': 'sendiri',\n",
       "  'sequence': 'mainan saya sendiri di jalan'},\n",
       " {'score': 0.029048316180706024,\n",
       "  'token': 2444,\n",
       "  'token_str': 'lahir',\n",
       "  'sequence': 'mainan saya lahir di jalan'},\n",
       " {'score': 0.028137197718024254,\n",
       "  'token': 3812,\n",
       "  'token_str': 'berdiri',\n",
       "  'sequence': 'mainan saya berdiri di jalan'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = transformers.pipeline('fill-mask', model = model_name)\n",
    "unmasker(\"mainan saya [MASK] di jalan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset json file\n",
    "import json\n",
    "\n",
    "train_json_dir = \"Translated/train-v2.0_indo.json\"\n",
    "dev_json_dir = \"Translated/dev-v2.0_indo.json\"\n",
    "tester_json_dir  = \"Translated/tester_indo.json\"\n",
    "\n",
    "dataset_dirs = [train_json_dir, dev_json_dir, tester_json_dir]\n",
    "# dataset_dirs = [tester_json_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function is for converting SQuAD json file to pandas dataframe, iteratively\n",
    "\n",
    "    I dont want run this locally, better use colab\n",
    "'''\n",
    "\n",
    "if ENABLE_JSON2CSV:\n",
    "    import utils\n",
    "    for dir in dataset_dirs:\n",
    "        with open(dir, encoding=\"utf-8\") as json_file:\n",
    "            file = json.load(json_file)\n",
    "            dict_file = file\n",
    "            data = dict_file['data']\n",
    "\n",
    "        df = utils.json_to_df(data)\n",
    "        df.to_csv(dir.replace(\".json\", \".csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384  # The maximum length of a feature (question and context)\n",
    "doc_stride = 128  # The allowed overlap between two part of the context when splitting is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Di negara manakah Normandia berada?</td>\n",
       "      <td>Perancis</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Di negara manakah Normandia berada?</td>\n",
       "      <td>Perancis</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Di negara manakah Normandia berada?</td>\n",
       "      <td>Perancis</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Di negara manakah Normandia berada?</td>\n",
       "      <td>Perancis</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56ddde6b9a695914005b9629</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Kapan orang Normandia di Normandia?</td>\n",
       "      <td>abad 10 dan 11</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   input_id            title  \\\n",
       "0  56ddde6b9a695914005b9628  orang Normandia   \n",
       "1  56ddde6b9a695914005b9628  orang Normandia   \n",
       "2  56ddde6b9a695914005b9628  orang Normandia   \n",
       "3  56ddde6b9a695914005b9628  orang Normandia   \n",
       "4  56ddde6b9a695914005b9629  orang Normandia   \n",
       "\n",
       "                                             context  \\\n",
       "0  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "1  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "2  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "3  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "4  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "\n",
       "                              question     answer_text  answer_start  \n",
       "0  Di negara manakah Normandia berada?        Perancis         159.0  \n",
       "1  Di negara manakah Normandia berada?        Perancis         159.0  \n",
       "2  Di negara manakah Normandia berada?        Perancis         159.0  \n",
       "3  Di negara manakah Normandia berada?        Perancis         159.0  \n",
       "4  Kapan orang Normandia di Normandia?  abad 10 dan 11          94.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    print(\"pandas is not installed\")\n",
    "    print(\"installing pandas...\")\n",
    "    %pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "train_csv_dir = \"Translated/train-v2.0_indo.csv\"\n",
    "dev_csv_dir = \"Translated/dev-v2.0_indo.csv\"\n",
    "tester_csv_dir  = \"Translated/tester_indo.csv\"\n",
    "\n",
    "# open one of the csv file\n",
    "df_tester = pd.read_csv(tester_csv_dir)\n",
    "df_tester.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated/train-v2.0_indo.csv has no features longer than 384\n",
      "\n",
      "\n",
      "Translated/dev-v2.0_indo.csv has no features longer than 384\n",
      "\n",
      "\n",
      "Translated/tester_indo.csv has no features longer than 384\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dirs = [train_csv_dir, dev_csv_dir, tester_csv_dir]\n",
    "\n",
    "# check every csv file, if there is feature longer than max_length, then print it\n",
    "for csv_dir in csv_dirs:\n",
    "    df = pd.read_csv(csv_dir)\n",
    "    longer_feature = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if len(row[['context', 'question', 'input_id']]) > max_length:\n",
    "            longer_feature += 1\n",
    "            print(f\"{csv_dir} has {longer_feature} features longer than {max_length}\")\n",
    "            print(\"\\n\")\n",
    "    if longer_feature == 0:\n",
    "        print(f\"{csv_dir} has no features longer than {max_length}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some layers of TFBertForQuestionAnswering were not initialized from the model checkpoint at cahya/bert-base-indonesian-522M and are newly initialized: ['qa_outputs']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "num_train_epochs = 2\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Arya Wijna\\AppData\\Local\\Temp\\ipykernel_5716\\3891755714.py:6: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: repair this\n",
    "def prepare_train_features(train):\n",
    "    '''Extract feature from train dataset\n",
    "\n",
    "    Args:\n",
    "        train (pandas dataframe): train dataset\n",
    "\n",
    "    Returns:\n",
    "        tokenized_train_features (list): list of tokenized train features\n",
    "    '''\n",
    "    tokenized_df = tokenizer(\n",
    "        train['context'].tolist(),\n",
    "        train['question'].tolist(),\n",
    "        add_special_tokens=True, # add [CLS], [SEP]\n",
    "        max_length=max_length, # max length of feature\n",
    "        stride=doc_stride, # allowed overlap between two part of the context when splitting is performed\n",
    "        truncation_strategy='only_second', # truncate the first part of the context\n",
    "        padding='max_length', # padding strategy\n",
    "        return_tensors=\"tf\" # return tensorflow tensor\n",
    "    )\n",
    "\n",
    "    tokenized_train_features = [ tokenized_df ]\n",
    "\n",
    "    return tokenized_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:703\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=701'>702</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[1;32m--> <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=702'>703</a>\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=704'>705</a>\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=705'>706</a>\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=706'>707</a>\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=707'>708</a>\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=708'>709</a>\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=709'>710</a>\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=171'>172</a>\u001b[0m \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=172'>173</a>\u001b[0m \n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=173'>174</a>\u001b[0m \u001b[39mNote: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=264'>265</a>\u001b[0m \u001b[39m  ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=265'>266</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=266'>267</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=267'>268</a>\u001b[0m                       allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=277'>278</a>\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=278'>279</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=280'>281</a>\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n",
      "File \u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=302'>303</a>\u001b[0m \u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=303'>304</a>\u001b[0m t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=100'>101</a>\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/tensorflow/python/framework/constant_op.py?line=101'>102</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\ML\\distilbert_transfer_learn.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000020?line=0'>1</a>\u001b[0m features \u001b[39m=\u001b[39m prepare_train_features(df_tester)\n",
      "\u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\ML\\distilbert_transfer_learn.ipynb Cell 20'\u001b[0m in \u001b[0;36mprepare_train_features\u001b[1;34m(train)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=1'>2</a>\u001b[0m \u001b[39m'''Extract feature from train dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=2'>3</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=3'>4</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=7'>8</a>\u001b[0m \u001b[39m    tokenized_train_features (list): list of tokenized train features\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=8'>9</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=9'>10</a>\u001b[0m tokenized_train_features \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=10'>11</a>\u001b[0m tokenized_df \u001b[39m=\u001b[39m tokenizer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=11'>12</a>\u001b[0m     train[\u001b[39m'\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mtolist(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=12'>13</a>\u001b[0m     train[\u001b[39m'\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mtolist(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=13'>14</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=14'>15</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=15'>16</a>\u001b[0m     stride\u001b[39m=\u001b[39;49mdoc_stride,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=16'>17</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39monly_second\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=17'>18</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtf\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000019?line=18'>19</a>\u001b[0m )\n",
      "File \u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2455\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2450'>2451</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2451'>2452</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2452'>2453</a>\u001b[0m         )\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2453'>2454</a>\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[1;32m-> <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2454'>2455</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2455'>2456</a>\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2456'>2457</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2457'>2458</a>\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2458'>2459</a>\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2459'>2460</a>\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2460'>2461</a>\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2461'>2462</a>\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2462'>2463</a>\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2463'>2464</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2464'>2465</a>\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2465'>2466</a>\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2466'>2467</a>\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2467'>2468</a>\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2468'>2469</a>\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2469'>2470</a>\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2470'>2471</a>\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2471'>2472</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2472'>2473</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2473'>2474</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2474'>2475</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2475'>2476</a>\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2476'>2477</a>\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2492'>2493</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2493'>2494</a>\u001b[0m     )\n",
      "File \u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2646\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2635'>2636</a>\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2636'>2637</a>\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2637'>2638</a>\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2638'>2639</a>\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2642'>2643</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2643'>2644</a>\u001b[0m )\n\u001b[1;32m-> <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2645'>2646</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2646'>2647</a>\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2647'>2648</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2648'>2649</a>\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2649'>2650</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2650'>2651</a>\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2651'>2652</a>\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2652'>2653</a>\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2653'>2654</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2654'>2655</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2655'>2656</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2656'>2657</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2657'>2658</a>\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2658'>2659</a>\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2659'>2660</a>\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2660'>2661</a>\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2661'>2662</a>\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2662'>2663</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2663'>2664</a>\u001b[0m )\n",
      "File \u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py:734\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=730'>731</a>\u001b[0m     second_ids \u001b[39m=\u001b[39m get_input_ids(pair_ids) \u001b[39mif\u001b[39;00m pair_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=731'>732</a>\u001b[0m     input_ids\u001b[39m.\u001b[39mappend((first_ids, second_ids))\n\u001b[1;32m--> <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=733'>734</a>\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_prepare_for_model(\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=734'>735</a>\u001b[0m     input_ids,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=735'>736</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=736'>737</a>\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=737'>738</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=738'>739</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=739'>740</a>\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=740'>741</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=741'>742</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=742'>743</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=743'>744</a>\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=744'>745</a>\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=745'>746</a>\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=746'>747</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=747'>748</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=748'>749</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=750'>751</a>\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py:814\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[1;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=803'>804</a>\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=805'>806</a>\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad(\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=806'>807</a>\u001b[0m     batch_outputs,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=807'>808</a>\u001b[0m     padding\u001b[39m=\u001b[39mpadding_strategy\u001b[39m.\u001b[39mvalue,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=810'>811</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=811'>812</a>\u001b[0m )\n\u001b[1;32m--> <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=813'>814</a>\u001b[0m batch_outputs \u001b[39m=\u001b[39m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils.py?line=815'>816</a>\u001b[0m \u001b[39mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:208\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=203'>204</a>\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=205'>206</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[1;32m--> <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=207'>208</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:719\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=713'>714</a>\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=714'>715</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=715'>716</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=716'>717</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=717'>718</a>\u001b[0m             )\n\u001b[1;32m--> <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=718'>719</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=719'>720</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=720'>721</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwith \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=721'>722</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=723'>724</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "features = prepare_train_features(df_tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load preprocessed dataset\n",
    "# TODO: train model\n",
    "# TODO: evaluate model\n",
    "# TODO: save model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7f8b4dcb172d5ee7cbc6306f910f9536d12fce62ed018c909e0bf052dc4ebfc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
