{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# check python version\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook settings\n",
    "\n",
    "COLAB_MODE = False # set to True if running in Google Colab\n",
    "ENABLE_JSON2CSV = False # set to True if you want to convert json dataset to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if COLAB_MODE is True, then work around the repository\n",
    "if COLAB_MODE:\n",
    "    import os\n",
    "    branch_name = 'dhupee-dev'\n",
    "    cloned_repo_name = 'remote-clone'\n",
    "    target_repo_dir = '/content/remote-clone/ML'\n",
    "    repo_link = 'https://github.com/dhupee/Bangkit-C22CB-Company-Based-Capstone.git'\n",
    "    # if current directory is not the cloned repo, clone it\n",
    "    if not os.path.exists(target_repo_dir):\n",
    "        !git clone --single-branch --branch $branch_name $repo_link $cloned_repo_name\n",
    "        print('Repo successfully cloned!')\n",
    "        %cd $target_repo_dir\n",
    "        %pwd\n",
    "    else:\n",
    "        print('Repo already cloned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers and tensorflow are installed\n"
     ]
    }
   ],
   "source": [
    "# check if transformers and tensorflow are installed, if not install them\n",
    "# use transformers version 4.18.0 and tensorflow version 2.8.0\n",
    "try:\n",
    "    import transformers\n",
    "    import tensorflow as tf\n",
    "    print(\"transformers and tensorflow are installed\")\n",
    "except:\n",
    "    print(\"transformers and tensorflow are not installed\")\n",
    "    print(\"installing transformers and tensorflow\")\n",
    "    # install transformers 4.18.0 and tensorflow 2.8.0\n",
    "    %pip install transformers==4.18.0\n",
    "    %pip install tensorflow==2.8.0\n",
    "    # import transformers and tensorflow again\n",
    "    import transformers\n",
    "    import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at cahya/bert-base-indonesian-522M were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cahya/bert-base-indonesian-522M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"cahya/bert-base-indonesian-522M\"\n",
    "batch_size = 32\n",
    "\n",
    "from transformers import BertTokenizer, TFAutoModel # make sure use tensorflow model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  110617344 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,617,344\n",
      "Trainable params: 110,617,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 1769, 8343, 6186, 32, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tokenizer\n",
    "tokenizer(\"Nama kamu siapa?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 3245, 5366, 2464, 6014, 11186, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"saya suka makan nasi goreng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at cahya/bert-base-indonesian-522M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0840364545583725,\n",
       "  'token': 2186,\n",
       "  'token_str': 'berada',\n",
       "  'sequence': 'mainan saya berada di jalan'},\n",
       " {'score': 0.07038316130638123,\n",
       "  'token': 1821,\n",
       "  'token_str': 'ada',\n",
       "  'sequence': 'mainan saya ada di jalan'},\n",
       " {'score': 0.0403575636446476,\n",
       "  'token': 1998,\n",
       "  'token_str': 'sendiri',\n",
       "  'sequence': 'mainan saya sendiri di jalan'},\n",
       " {'score': 0.029048316180706024,\n",
       "  'token': 2444,\n",
       "  'token_str': 'lahir',\n",
       "  'sequence': 'mainan saya lahir di jalan'},\n",
       " {'score': 0.028137197718024254,\n",
       "  'token': 3812,\n",
       "  'token_str': 'berdiri',\n",
       "  'sequence': 'mainan saya berdiri di jalan'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = transformers.pipeline('fill-mask', model = model_name)\n",
    "unmasker(\"mainan saya [MASK] di jalan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset json file\n",
    "import json\n",
    "\n",
    "train_json_dir = \"Translated/train-v2.0_indo.json\"\n",
    "dev_json_dir = \"Translated/dev-v2.0_indo.json\"\n",
    "tester_json_dir  = \"Translated/tester_indo.json\"\n",
    "\n",
    "dataset_dirs = [train_json_dir, dev_json_dir, tester_json_dir]\n",
    "# dataset_dirs = [tester_json_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function is for converting SQuAD json file to pandas dataframe, iteratively\n",
    "\n",
    "    I dont want run this locally, better use colab\n",
    "'''\n",
    "\n",
    "if ENABLE_JSON2CSV:\n",
    "    import utils\n",
    "    for dir in dataset_dirs:\n",
    "        with open(dir, encoding=\"utf-8\") as json_file:\n",
    "            file = json.load(json_file)\n",
    "            dict_file = file\n",
    "            data = dict_file['data']\n",
    "\n",
    "        df = utils.json_to_df(data)\n",
    "        df.to_csv(dir.replace(\".json\", \".csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384  # The maximum length of a feature (question and context)\n",
    "doc_stride = 128  # The allowed overlap between two part of the context when splitting is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Di negara manakah Normandia berada?</td>\n",
       "      <td>Perancis</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Di negara manakah Normandia berada?</td>\n",
       "      <td>Perancis</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Di negara manakah Normandia berada?</td>\n",
       "      <td>Perancis</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Di negara manakah Normandia berada?</td>\n",
       "      <td>Perancis</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56ddde6b9a695914005b9629</td>\n",
       "      <td>orang Normandia</td>\n",
       "      <td>Gaya pound memiliki padanan metrik, lebih jara...</td>\n",
       "      <td>Kapan orang Normandia di Normandia?</td>\n",
       "      <td>abad 10 dan 11</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   input_id            title  \\\n",
       "0  56ddde6b9a695914005b9628  orang Normandia   \n",
       "1  56ddde6b9a695914005b9628  orang Normandia   \n",
       "2  56ddde6b9a695914005b9628  orang Normandia   \n",
       "3  56ddde6b9a695914005b9628  orang Normandia   \n",
       "4  56ddde6b9a695914005b9629  orang Normandia   \n",
       "\n",
       "                                             context  \\\n",
       "0  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "1  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "2  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "3  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "4  Gaya pound memiliki padanan metrik, lebih jara...   \n",
       "\n",
       "                              question     answer_text  answer_start  \n",
       "0  Di negara manakah Normandia berada?        Perancis         159.0  \n",
       "1  Di negara manakah Normandia berada?        Perancis         159.0  \n",
       "2  Di negara manakah Normandia berada?        Perancis         159.0  \n",
       "3  Di negara manakah Normandia berada?        Perancis         159.0  \n",
       "4  Kapan orang Normandia di Normandia?  abad 10 dan 11          94.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    print(\"pandas is not installed\")\n",
    "    print(\"installing pandas...\")\n",
    "    %pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "train_csv_dir = \"Translated/train-v2.0_indo.csv\"\n",
    "dev_csv_dir = \"Translated/dev-v2.0_indo.csv\"\n",
    "tester_csv_dir  = \"Translated/tester_indo.csv\"\n",
    "\n",
    "# open one of the csv file\n",
    "df_tester = pd.read_csv(tester_csv_dir)\n",
    "df_tester.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input_id         object\n",
       "title            object\n",
       "context          object\n",
       "question         object\n",
       "answer_text      object\n",
       "answer_start    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check dataframe datatype\n",
    "df_tester.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated/train-v2.0_indo.csv has no features longer than 384\n",
      "\n",
      "\n",
      "Translated/dev-v2.0_indo.csv has no features longer than 384\n",
      "\n",
      "\n",
      "Translated/tester_indo.csv has no features longer than 384\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dirs = [train_csv_dir, dev_csv_dir, tester_csv_dir]\n",
    "\n",
    "# check every csv file, if there is feature longer than max_length, then print it\n",
    "for csv_dir in csv_dirs:\n",
    "    df = pd.read_csv(csv_dir)\n",
    "    longer_feature = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if len(row[['context', 'question', 'input_id']]) > max_length:\n",
    "            longer_feature += 1\n",
    "            print(f\"{csv_dir} has {longer_feature} features longer than {max_length}\")\n",
    "            print(\"\\n\")\n",
    "    if longer_feature == 0:\n",
    "        print(f\"{csv_dir} has no features longer than {max_length}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some layers of TFBertForQuestionAnswering were not initialized from the model checkpoint at cahya/bert-base-indonesian-522M and are newly initialized: ['qa_outputs']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "num_train_epochs = 2\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Arya Wijna\\AppData\\Local\\Temp\\ipykernel_5716\\3891755714.py:6: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    \n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples['context'],\n",
    "        examples['question'],\n",
    "        add_special_tokens=True, # add [CLS], [SEP]\n",
    "        max_length=max_length, # max length of feature\n",
    "        stride=doc_stride, # allowed overlap between two part of the context when splitting is performed\n",
    "        truncation_strategy='only_second', # truncate the first part of the context\n",
    "        padding='max_length', # padding strategy\n",
    "        return_tensors=\"tf\" # return tensorflow tensor\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (\n",
    "                offsets[token_start_index][0] <= start_char\n",
    "                and offsets[token_end_index][1] >= end_char\n",
    "            ):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while (\n",
    "                    token_start_index < len(offsets)\n",
    "                    and offsets[token_start_index][0] <= start_char\n",
    "                ):\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\ML\\distilbert_transfer_learn.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000020?line=0'>1</a>\u001b[0m features \u001b[39m=\u001b[39m prepare_train_features(df_tester)\n",
      "\u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\ML\\distilbert_transfer_learn.ipynb Cell 20'\u001b[0m in \u001b[0;36mprepare_train_features\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_train_features\u001b[39m(examples):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=1'>2</a>\u001b[0m     \u001b[39m# Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=2'>3</a>\u001b[0m     \u001b[39m# in one example possible giving several features when a context is long, each of those features having a\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=3'>4</a>\u001b[0m     \u001b[39m# context that overlaps a bit the context of the previous feature.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=5'>6</a>\u001b[0m     pad_on_right \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mpadding_side \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mright\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=7'>8</a>\u001b[0m     tokenized_examples \u001b[39m=\u001b[39m tokenizer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=8'>9</a>\u001b[0m         examples[\u001b[39m'\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=9'>10</a>\u001b[0m         examples[\u001b[39m'\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=10'>11</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m# add [CLS], [SEP]\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=11'>12</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length, \u001b[39m# max length of feature\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=12'>13</a>\u001b[0m         stride\u001b[39m=\u001b[39;49mdoc_stride, \u001b[39m# allowed overlap between two part of the context when splitting is performed\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=13'>14</a>\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39monly_second\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m# truncate the first part of the context\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=14'>15</a>\u001b[0m         padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m# padding strategy\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=15'>16</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtf\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m# return tensorflow tensor\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=16'>17</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=18'>19</a>\u001b[0m     \u001b[39m# Since one example might give us several features if it has a long context, we need a map from a feature to\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=19'>20</a>\u001b[0m     \u001b[39m# its corresponding example. This key gives us just that.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/ML/distilbert_transfer_learn.ipynb#ch0000022?line=20'>21</a>\u001b[0m     sample_mapping \u001b[39m=\u001b[39m tokenized_examples\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39moverflow_to_sample_mapping\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\DH\\projects\\Bangkit-C22CB-Company-Based-Capstone\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2429\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2425'>2426</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2427'>2428</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2428'>2429</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2429'>2430</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2430'>2431</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2431'>2432</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2433'>2434</a>\u001b[0m \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2434'>2435</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2435'>2436</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2436'>2437</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///d%3A/DH/projects/Bangkit-C22CB-Company-Based-Capstone/venv/lib/site-packages/transformers/tokenization_utils_base.py?line=2437'>2438</a>\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "features = prepare_train_features(df_tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load preprocessed dataset\n",
    "# TODO: train model\n",
    "# TODO: evaluate model\n",
    "# TODO: save model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7f8b4dcb172d5ee7cbc6306f910f9536d12fce62ed018c909e0bf052dc4ebfc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
